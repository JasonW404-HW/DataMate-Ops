import os
import json
from typing import Dict, Any

from loguru import logger
import httpx
from pathlib import Path

from datamate.core.base_op import Mapper

import pandas as pd


# Custom logger format: Add ğŸŸ§ğŸŸ§ğŸŸ§ to all log startings and endings.
class OpsLogger:
    def __init__(self):
        self.logger = logger
        self.prefix = "ğŸŸ§ğŸŸ§ğŸŸ§"
        self.appendix = "ğŸŸ¦ğŸŸ¦ğŸŸ¦"

    def debug(self, message: str):
        self.logger.debug(f"{self.prefix} {message} {self.appendix}")

    def info(self, message: str):
        self.logger.info(f"{self.prefix} {message} {self.appendix}")

    def warning(self, message: str):
        self.logger.warning(f"{self.prefix} {message} {self.appendix}")

    def error(self, message: str):
        self.logger.error(f"{self.prefix} {message} {self.appendix}")

ops_logger = OpsLogger()

class PathoSysPreprocess(Mapper):
    """
    ç—…ç†ç³»ç»Ÿæ•°æ®é¢„å¤„ç†ç®—å­
    """

    def __init__(self, *args, **kwargs):
        """
        åˆå§‹åŒ–å‚æ•°
        :param args:
        :param kwargs:
        """
        super().__init__(*args, **kwargs)

        # è·¯å¾„çš„è½¬æ¢é…ç½®ï¼Œæ”¯æŒå¦‚ä¸‹ä¸‰ç§æ¨¡å¼ï¼š
        # 1. ä¿æŒä¸å˜ï¼š"<>"ï¼ˆéœ€è¦ä½¿ç”¨è¿™ä¸ªç‰¹æ®Šç¬¦å·çš„ç»„åˆï¼Œè€Œä¸æ˜¯ç©ºå­—ç¬¦ä¸²ï¼Œå› ä¸ºç©ºå­—ç¬¦ä¸²ä¼šå¯¼è‡´å‰ç«¯è®¤ä¸ºæœªå¡«å¿…å¡«é¡¹ï¼‰
        # 2. æºè·¯å¾„ä¸ºç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦æ ¹æ®æŒ‚è½½ç‚¹è¿›è¡Œè¡¥å…¨ï¼š"<æŒ‚è½½ç‚¹ç»å¯¹è·¯å¾„>", é»˜è®¤å€¼ä¸º"/mnt/ruipath/hospital_data/"
        # 3. æºè·¯å¾„ä¸ºç»å¯¹è·¯å¾„ï¼Œæˆ–å­˜åœ¨å‰ç¼€æ›¿æ¢éœ€æ±‚çš„ï¼š"<åŸå‰ç¼€>:<æ–°å‰ç¼€>"ï¼Œå¦‚ "storage/:/mnt/ruipath/hospital_data/"
        # æ­¤é…ç½®é¡¹çš„é»˜è®¤é…ç½®ä¸ºç¬¬äºŒç§æ¨¡å¼ï¼Œ
        self.path_transformer = Path(kwargs.get('pathTransformer', '/mnt/ruipath/hospital_data/'))

        self.ignore_sdpc = kwargs.get('ignoreSdpc', False)

    def extra_filters(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        é¢å¤–çš„è¿‡æ»¤æ¡ä»¶
        :param df: è¾“å…¥çš„ DataFrame
        :return: è¿‡æ»¤åçš„ DataFrame
        """
        # ç¤ºä¾‹ï¼šæ ¹æ®æŸåˆ—å€¼è¿›è¡Œè¿‡æ»¤
        # df = df[df['some_column'] > threshold_value]
        return df

    def execute(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¸å¿ƒå¤„ç†é€»è¾‘
        :param sample: è¾“å…¥çš„æ•°æ®æ ·æœ¬ï¼Œé€šå¸¸åŒ…å« text_key ç­‰å­—æ®µ
        :return: å¤„ç†åçš„æ•°æ®æ ·æœ¬
        """

        file_path = sample.get('filePath', None)
        if not file_path:
            raise ValueError("Sample must contain valid 'filePath' key and value.")
        if not isinstance(file_path, str):
            raise TypeError("'filePath' must be a string.")
        if not file_path.endswith('.csv'):
            raise ValueError("'filePath' must point to a CSV file.")
        
        diagnosis_file_path = Path(file_path)
        diagnosis_file_name = diagnosis_file_path.name
        diagnosis_file_dir = diagnosis_file_path.parent

        ops_logger.info(f"Processing file: {diagnosis_file_path}")
        
        # >>> è¯»å–åŒ…å« diagnosis çš„ CSV æ–‡ä»¶
        #     -----------------------------
        diagnosis_df = pd.read_csv(diagnosis_file_path)
        if not all(col in diagnosis_df.columns for col in ["case_no", "diagnosis"]):
            return sample

        # >>> è¯»å–åŒ…å« slide_path çš„ CSV æ–‡ä»¶ 
        #     ------------------------------
        try:
            slide_file_path = [f for f in os.listdir(diagnosis_file_dir) if f != diagnosis_file_name][0]
        except IndexError:
            ops_logger.error(f"No slide CSV file found in the directory.")
            return sample
        
        slide_info_df = pd.read_csv(os.path.join(diagnosis_file_dir, slide_file_path))
        if not all(col in slide_info_df.columns for col in ["case_no", "slide_path"]):
            return sample
        if not "thumbnail_path" in slide_info_df.columns:
            ops_logger.warning(f"No 'thumbnail_path' column found in slide CSV file. All SPDC files will be ignored.")
            self.ignore_sdpc = True

        ops_logger.info(f"File read: Diagnosis CSV: {diagnosis_df.shape}")
        ops_logger.info(f"File read: Slide CSV:     {slide_info_df.shape}")

        # >>> åˆå¹¶ DataFrame 
        #     --------------

        merged_df = pd.merge(diagnosis_df, slide_info_df, on="case_no", how="inner")

        ops_logger.info(f"Data merged: {merged_df.shape}")

        # >>> æ•°æ®å¤„ç†
        #     -------
        try:
            merged_df = self.data_processing(merged_df)
        except Exception as e:
            ops_logger.error(f"Data processing failed: {e}")
            return sample
        
        ops_logger.info(f"Data processed: {merged_df.shape}")

        # >>> æ’å…¥æ•°æ®è®°å½•åˆ°æ•°æ®é›†
        #     ------------------
        try:
            export_path = sample.get('export_path', None)
            if not export_path:
                ops_logger.error(f"Sample missing 'export_path' key or value.")
                raise ValueError("Sample must contain valid 'export_path' key and value.")

            merged_df = self.insert_into_dataset(merged_df, Path(export_path))

        except Exception as e:
            ops_logger.error(f"Failed to insert records into dataset: {e}")
            return sample
        
        ops_logger.info(f"Data inserted into dataset: {merged_df.shape}")

        # >>> æ›´æ–° sample
        #     -----------
        sample["text"] = merged_df.to_json(orient="records", force_ascii=False, indent=2)
        sample["fileName"] = f"case_diagnosis_slides.json"
        sample["fileType"] = "json"

        ops_logger.info(f"Sample updated with processed data.")

        return sample
    
    def data_processing(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ•°æ®å¤„ç†é€»è¾‘
        :param df: è¾“å…¥çš„ DataFrame
        :return: å¤„ç†åçš„ DataFrame
        """

        # >>> å¤„ç†éæ³•å€¼
        #     ---------

        # slide_path ä¸ºç©ºå€¼çš„è¡Œ
        df = df[df["slide_path"].notna() & (df["slide_path"] != "")]

        # >>> å¤„ç† SDPC æ–‡ä»¶
        #     -------------
        if self.ignore_sdpc:
            df = df[~df["slide_path"].str.endswith('.sdpc')]
        else:
            df = df[~((df["slide_path"].str.endswith('.sdpc')) & 
                    (df["thumbnail_path"].isna() | (df["thumbnail_path"] == "")))]
            
        # >>> å¤„ç†æ–‡ä»¶è·¯å¾„è½¬æ¢
        #     ---------------
        df[["slide_path", "thumbnail_path"]] = df.apply(
            lambda row: (
                self.transform_path(row["slide_path"]),
                self.transform_path(row["thumbnail_path"]) if pd.notna(row["thumbnail_path"]) else ""
            ), axis=1, result_type='expand'
        )
            
        # >>> å…¶ä»–è¿‡æ»¤æ¡ä»¶
        #     -----------
        df = self.extra_filters(df)

        return df
    
    def transform_path(self, known_path: str) -> str:
        """
        Adjusts a known path based on user input.
        - Empty string: returns original.
        - 'old:new': replaces prefix.
        - 'path': prepends as parent.
        """
        transform_rule = str(self.path_transformer)

        # æ¨¡å¼3ï¼šä¿æŒä¸å˜
        if not transform_rule.strip():
            return known_path

        known_p = Path(known_path)
        
        # æ¨¡å¼2ï¼šæ›¿æ¢å‰ç¼€ï¼ˆæŒ‚è½½ç‚¹å˜æ›´ï¼‰
        if ":" in transform_rule:
            try:
                old_prefix, new_prefix = transform_rule.split(":", 1)
                # Convert to string to check start, then reconstruct
                path_str = str(known_p)
                if path_str.startswith(old_prefix):
                    # Replace the prefix and ensure it's a valid path again
                    updated_str = path_str.replace(old_prefix, new_prefix, 1)
                    return str(Path(updated_str))
                else:
                    ops_logger.warning(f"Warning: Prefix '{old_prefix}' not found in '{known_path}'")
                    return known_path
            except ValueError:
                ops_logger.error("Error: Invalid mapping format. Use 'old:new'")
                return known_path

        # æ¨¡å¼1ï¼šä½œä¸ºçˆ¶ç›®å½•æ·»åŠ ä¸ºå‰ç¼€
        new_parent = Path(transform_rule)
        return str(new_parent / known_p.relative_to(known_p.anchor) if known_p.is_absolute() else new_parent / known_p)
    
    def insert_into_dataset(self, merged_df: pd.DataFrame, export_path: Path) -> pd.DataFrame:
        """
        å°†åˆå¹¶åçš„æ•°æ®æ’å…¥åˆ°æ•°æ®é›†ä¸­ã€‚æ³¨æ„ï¼Œç¼©ç•¥å›¾å°†ä¾æ® slide_path è¿›è¡Œé‡å‘½åã€‚
        :param merged_df: åˆå¹¶åçš„ DataFrame
        :param export_path: ç›®æ ‡æ•°æ®é›†è·¯å¾„ï¼Œç”¨äºå­˜æ”¾ç—…ç†å›¾ç‰‡åŠå…¶ç¼©ç•¥å›¾ï¼ˆå¯é€‰ï¼‰çš„è½¯é“¾æ¥
        """

        # æ’å…¥æ•°æ®é›† - DataMate å†…éƒ¨ API è°ƒç”¨
        API_URL = f"http://datamate-backend:8080/api/data-management/datasets/{export_path.name}/files/upload/add"
        BATCH_SIZE = 1000

        ops_logger.info(f"DataMate connection API URL: {API_URL} | Batch Size: {BATCH_SIZE}")

        for start_idx in range(0, len(merged_df), BATCH_SIZE):
            end_idx = min(start_idx + BATCH_SIZE, len(merged_df))
            batch_df = merged_df.iloc[start_idx:end_idx]
            
            # æ¯è¡Œæ•°æ®è½¬æ¢ä¸ºå­—å…¸ï¼š{"filePath": ..., "metadata": {...}}
            slide_records = []
            thumb_records = []
            for _, row in batch_df.iterrows():
                record = {"filePath": row["slide_path"]}
                metadata = row.drop(labels=["slide_path"]).to_dict()
                record["metadata"] = metadata

                slide_records.append(record)  # Slides File + Thumbnails Path + Metadata
                thumb_records.append({"filePath": row["thumbnail_path"]})  # Thumbnails File

            try:
                for t in ["slide", "thumbnail"]:
                    data = slide_records if t == "slide" else thumb_records
                    ops_logger.info(f"Uploading batch {start_idx}-{end_idx}: {len(data)} [[ {t} ]] records ")

                    request_body = {"files": data}
                    request_body_str = json.dumps(request_body, ensure_ascii=False)

                    curl_cmd = f'curl -X POST "{API_URL}" -H "Content-Type: application/json" -d \'{request_body_str}\' '
                    ops_logger.debug(f"Debug curl command for {t} records (batch {start_idx}-{end_idx}): {curl_cmd}")

                    response = httpx.post(API_URL, json={"files": data})
                    response.raise_for_status()

                    ops_logger.info(f"Successfully uploaded {t} records for batch {start_idx}-{end_idx}")

            except httpx.HTTPStatusError as e:
                response_body = e.response.text if e.response else "N/A"
                ops_logger.error(f"HTTP error uploading {t} records (batch {start_idx}-{end_idx}): {e}")
                ops_logger.error(f"Response body: {response_body}")
            except httpx.HTTPError as e:
                ops_logger.error(f"HTTP error uploading {t} records (batch {start_idx}-{end_idx}): {e}")
            except Exception as e:
                ops_logger.error(f"Failed to upload {t} records (batch {start_idx}-{end_idx}): {e}")

        return merged_df
